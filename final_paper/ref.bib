@article{https://doi.org/10.48550/arxiv.1904.07189,
  doi = {10.48550/ARXIV.1904.07189},
  url = {https://arxiv.org/abs/1904.07189},
  author = {Bouton, Maxime and Karlsson, Jesper and Nakhaei, Alireza and Fujimura, Kikuo and Kochenderfer, Mykel J. and Tumova, Jana},
  keywords = {Robotics (cs.RO), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Reinforcement Learning with Probabilistic Guarantees for Autonomous Driving},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{10.1145/3577204,
    author = {Boudi, Zakaryae and Wakrime, Abderrahim Ait and Toub, Mohamed and Haloua, Mohamed},
    title = {A Deep Reinforcement Learning Framework with Formal Verification},
    year = {2022},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    issn = {0934-5043},
    url = {https://doi.org/10.1145/3577204},
    doi = {10.1145/3577204},
    abstract = {Artificial Intelligence (AI) and data are reshaping organizations and businesses. Human Resources (HR) management and talent development make no exception, as they tend to involve more automation and growing quantities of data. Because this brings implications on workforce, career transparency and equal opportunities, overseeing what fuels AI and analytical models, their quality standards, integrity and correctness becomes an imperative for those aspiring to such systems. Based on an ontology transformation to B-machines, this paper presents an approach to constructing a valid and error-free career agent with Deep Reinforcement Learning (DRL). In short, the agent's policy is built on a framework we called Multi State-Actor (MuStAc) using a decentralised training approach. Its purpose is to predict both relevant and valid career steps to employees, based on their profiles and company pathways (observations). Observations can comprise various data elements such as the current occupation, past experiences, performance, skills and qualifications, etc. The policy takes in all these observations and outputs the next recommended career step, in an environment set as the combination of a HR ontology and an Event-B model, which generates action spaces with respect to formal properties. The Event-B model and formal properties are derived using OWL to B transformation.},
    note = {Just Accepted},
    journal = {Form. Asp. Comput.},
    month = {dec}
}

@ARTICLE{9357415,
  author={Wang, Xiaoyan and Peng, Jun and Li, Shuqiu and Li, Bing},
  journal={IEEE Access}, 
  title={Formal Reachability Analysis for Multi-Agent Reinforcement Learning Systems}, 
  year={2021},
  volume={9},
  number={},
  pages={45812-45821},
  doi={10.1109/ACCESS.2021.3060156}
}


@misc{https://doi.org/10.48550/arxiv.1801.08099,
  doi = {10.48550/ARXIV.1801.08099},
  url = {https://arxiv.org/abs/1801.08099},
  author = {Hasanbeig, Mohammadhosein and Abate, Alessandro and Kroening, Daniel},
  keywords = {Machine Learning (cs.LG), Logic in Computer Science (cs.LO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Logically-Constrained Reinforcement Learning},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@INPROCEEDINGS{9287929,
  author={Corsi, Davide and Marchesini, Enrico and Farinelli, Alessandro and Fiorini, Paolo},
  booktitle={2020 Fourth IEEE International Conference on Robotic Computing (IRC)}, 
  title={Formal Verification for Safe Deep Reinforcement Learning in Trajectory Generation}, 
  year={2020},
  volume={},
  number={},
  pages={352-359},
  doi={10.1109/IRC.2020.00062}
}

@INPROCEEDINGS{9724872,
  author={Mindom, Paulina Stevia Nouwou and Nikanjam, Amin and Khomh, Foutse and Mullins, John},
  booktitle={2021 IEEE 21st International Conference on Software Quality, Reliability and Security (QRS)}, 
  title={On Assessing The Safety of Reinforcement Learning algorithms Using Formal Methods}, 
  year={2021},
  volume={},
  number={},
  pages={260-269},
  doi={10.1109/QRS54544.2021.00037}}

@misc{https://doi.org/10.48550/arxiv.1509.02971,
  doi = {10.48550/ARXIV.1509.02971},
  url = {https://arxiv.org/abs/1509.02971},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Continuous control with deep reinforcement learning},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@InProceedings{10.1007/978-3-319-41540-6_17,
author="Sickert, Salomon
and Esparza, Javier
and Jaax, Stefan
and K{\v{r}}et{\'i}nsk{\'y}, Jan",
editor="Chaudhuri, Swarat
and Farzan, Azadeh",
title="Limit-Deterministic B{\"u}chi Automata for Linear Temporal Logic",
booktitle="Computer Aided Verification",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="312--332",
abstract="Limit-deterministic B{\"u}chi automata can replace deterministic Rabin automata in probabilistic model checking algorithms, and can be significantly smaller. We present a direct construction from an LTL formula {\$}{\$}{\backslash}varphi {\$}{\$}to a limit-deterministic B{\"u}chi automaton. The automaton is the combination of a non-deterministic component, guessing the set of eventually true {\$}{\$}{\{}{\backslash}mathbf {\{}G{\}}{\}}{\$}{\$}-subformulas of {\$}{\$}{\backslash}varphi {\$}{\$}, and a deterministic component verifying this guess and using this information to decide on acceptance. Contrary to the indirect approach of constructing a non-deterministic automaton for {\$}{\$}{\backslash}varphi {\$}{\$}and then applying a semi-determinisation algorithm, our translation is compositional and has a clear logical structure. Moreover, due to its special structure, the resulting automaton can be used not only for qualitative, but also for quantitative verification of MDPs, using the same model checking algorithm as for deterministic automata. This allows one to reuse existing efficient implementations of this algorithm without any modification. Our construction yields much smaller automata for formulas with deep nesting of modal operators and performs at least as well as the existing approaches on general formulas.",
isbn="978-3-319-41540-6"
}

@book{russel2010,
  added-at = {2020-02-01T18:23:11.000+0100},
  author = {Russell, Stuart and Norvig, Peter},
  biburl = {https://www.bibsonomy.org/bibtex/20533b732950d1c5ab4ac12d4f32fe637/mialhoma},
  edition = 3,
  interhash = {53908a52dd4c6c8e39f93f4ffc8341be},
  intrahash = {0533b732950d1c5ab4ac12d4f32fe637},
  keywords = {ties4530},
  publisher = {Prentice Hall},
  timestamp = {2020-02-01T18:23:11.000+0100},
  title = {Artificial Intelligence: A Modern Approach},
  year = 2010
}

@inproceedings{10.1007/11564096_32,
author = {Riedmiller, Martin},
title = {Neural Fitted q Iteration – First Experiences with a Data Efficient Neural Reinforcement Learning Method},
year = {2005},
isbn = {3540292438},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11564096_32},
doi = {10.1007/11564096_32},
abstract = {This paper introduces NFQ, an algorithm for efficient and effective training of a Q-value function represented by a multi-layer perceptron. Based on the principle of storing and reusing transition experiences, a model-free, neural network based Reinforcement Learning algorithm is proposed. The method is evaluated on three benchmark problems. It is shown empirically, that reasonably few interactions with the plant are needed to generate control policies of high quality.},
booktitle = {Proceedings of the 16th European Conference on Machine Learning},
pages = {317–328},
numpages = {12},
location = {Porto, Portugal},
series = {ECML'05}
}

@misc{hasanbeig2022lcrl,
      title={LCRL: Certified Policy Synthesis via Logically-Constrained Reinforcement Learning}, 
      author={Hosein Hasanbeig and Daniel Kroening and Alessandro Abate},
      year={2022},
      eprint={2209.10341},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}